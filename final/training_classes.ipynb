{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "378b4272",
      "metadata": {
        "id": "378b4272"
      },
      "source": [
        "# Reinforcement Learning 2025 - Final Assignment\n",
        "\n",
        "**Authors:** Amit Ezer, Gal Yaacov Noy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e0ea80ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0ea80ba",
        "outputId": "2d5422a7-e9a5-47d7-f574-fb3d6bfeb840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: minigrid in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (3.0.0)\n",
            "Requirement already satisfied: gymnasium in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (1.2.0)\n",
            "Requirement already satisfied: matplotlib in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (3.10.3)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from minigrid) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from minigrid) (2.2.6)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (4.14.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: pillow>=8 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install minigrid gymnasium matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c3d95ce4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3d95ce4",
        "outputId": "a132b73b-56b6-451d-ce80-7e02127aeb9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Models will be saved to: ./models\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import minigrid\n",
        "from minigrid.wrappers import ImgObsWrapper, RGBImgPartialObsWrapper\n",
        "from collections import deque\n",
        "from abc import ABC, abstractmethod\n",
        "from datetime import datetime\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "MODELS_DIR = \"./models\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "print(f\"Models will be saved to: {MODELS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6YJeLgqj94t6",
      "metadata": {
        "id": "6YJeLgqj94t6"
      },
      "outputs": [],
      "source": [
        "class MiniGridCNN(nn.Module):\n",
        "    def __init__(self, output_dim=128, input_channels=3, input_size=84):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Compute the output size after the conv layers by passing a dummy input.\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, input_channels, input_size, input_size)\n",
        "            conv_out_size = self.conv(dummy_input).shape[1]\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "o02qXjJ4-MTC",
      "metadata": {
        "id": "o02qXjJ4-MTC"
      },
      "outputs": [],
      "source": [
        "class BaseAgent(ABC):\n",
        "    def __init__(self, env_name, num_episodes=1000, max_steps=500):\n",
        "        self.env_name = env_name\n",
        "        self.env = self._create_env(env_name)\n",
        "        self.num_actions = self.env.action_space.n\n",
        "        self.obs_shape = self.env.reset()[0].shape\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "        self.losses = []\n",
        "\n",
        "    def _preprocess(self, obs):\n",
        "        \"\"\"\n",
        "        Convert RGB observation to tensor and normalize to [0,1] range.\n",
        "        Input: RGB image array (H, W, 3)\n",
        "        Output: tensor (C, H, W) normalized to [0,1]\n",
        "        \"\"\"\n",
        "        return torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "\n",
        "    def _create_env(self, env_name):\n",
        "        \"\"\"Create and return a MiniGrid environment with RGB observations.\"\"\"\n",
        "        env = gym.make(env_name)\n",
        "        env = RGBImgPartialObsWrapper(env)\n",
        "        env = ImgObsWrapper(env)\n",
        "        return env\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_algorithm_name(self):\n",
        "        \"\"\"Return string identifier of the algorithm.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _save_model(self, save_dir: str):\n",
        "        \"\"\"Save model to the specified directory.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def save_model(self, models_dir=MODELS_DIR):\n",
        "        \"\"\"Save trained model specified directory.\"\"\"\n",
        "        env_clean = self.env_name.replace(\"MiniGrid-\", \"\").replace(\"-v0\", \"\")\n",
        "        algorithm_name = self._get_algorithm_name()\n",
        "        \n",
        "        save_dir = os.path.join(models_dir, f\"{algorithm_name}_{env_clean}\")\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        \n",
        "        self._save_model(save_dir)\n",
        "        return save_dir\n",
        "    \n",
        "    def _get_additional_logs_str(self):\n",
        "        \"\"\"Return additional logs as a formatted string for printing.\"\"\"\n",
        "        return \"\"\n",
        "    \n",
        "    def get_additional_logs(self):\n",
        "        \"\"\"Return any additional logs to be printed during training.\"\"\"\n",
        "        return {}\n",
        "\n",
        "    \n",
        "    @abstractmethod\n",
        "    def _train_episode(self, episode: int):\n",
        "        \"\"\"Runs and trains a single episode. Should return (reward, steps, loss).\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def train(self):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TRAINING {self._get_algorithm_name().upper()} ON {self.env.spec.id.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Episodes: {self.num_episodes} | Max Steps: {self.max_steps}\")\n",
        "        print(f\"Observation Shape: {self.obs_shape} | Action Space: {self.num_actions}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        for episode in range(self.num_episodes):\n",
        "            reward, steps, loss = self._train_episode(episode)\n",
        "            self.episode_rewards.append(reward)\n",
        "            self.episode_lengths.append(steps)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            if episode % 100 == 0:\n",
        "                recent_rewards = np.mean(self.episode_rewards[-50:]) if len(self.episode_rewards) >= 50 else np.mean(self.episode_rewards)\n",
        "                recent_steps = np.mean(self.episode_lengths[-50:]) if len(self.episode_lengths) >= 50 else np.mean(self.episode_lengths)\n",
        "                recent_loss = np.mean(self.losses[-100:]) if len(self.losses) >= 100 else (np.mean(self.losses) if self.losses else 0)\n",
        "                print(f\"[{self._get_algorithm_name()}] Ep {episode:4d} | Reward: {reward:6.2f} | Avg Reward: {recent_rewards:6.2f} | Steps: {steps:3d} | Avg Steps: {recent_steps:5.1f} | Loss: {recent_loss:.4f}{self._get_additional_logs_str()}\")\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TRAINING COMPLETED!\")\n",
        "        final_rewards = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n",
        "        final_steps = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)\n",
        "        print(f\"Final Average Reward (last 100 episodes): {final_rewards:.3f}\")\n",
        "        print(f\"Final Average Steps (last 100 episodes): {final_steps:.1f}\")\n",
        "        print(f\"Total Training Episodes: {len(self.episode_rewards)}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        self.env.close()\n",
        "        \n",
        "        return {\n",
        "            \"name\": self._get_algorithm_name(),\n",
        "            \"rewards\": self.episode_rewards,\n",
        "            \"steps\": self.episode_lengths,\n",
        "            \"losses\": self.losses,\n",
        "            **self.get_additional_logs()\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fIDmHnhbBBNC",
      "metadata": {
        "id": "fIDmHnhbBBNC"
      },
      "outputs": [],
      "source": [
        "def plot_training_results(agent, window=50):\n",
        "    \"\"\"Plot training results with moving averages.\"\"\"\n",
        "    def moving_average(data, window):\n",
        "        if len(data) < window:\n",
        "            return data\n",
        "        return np.convolve(data, np.ones(window) / window, mode=\"valid\")\n",
        "\n",
        "    rewards = agent.episode_rewards\n",
        "    steps = agent.episode_lengths\n",
        "    losses = agent.losses\n",
        "    \n",
        "    # Get additional logs through the proper inheritance method\n",
        "    additional_logs = agent.get_additional_logs()\n",
        "    epsilons = additional_logs.get(\"epsilons\", [])\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle(f\"{agent.__class__.__name__} Training Results\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Episode Rewards\n",
        "    axes[0, 0].plot(rewards, alpha=0.3, label=\"Raw\", color='blue')\n",
        "    if len(rewards) >= window:\n",
        "        smooth_rewards = moving_average(rewards, window)\n",
        "        axes[0, 0].plot(range(window-1, len(rewards)), smooth_rewards,\n",
        "                        label=f\"Moving Avg ({window})\", linewidth=2, color='red')\n",
        "    axes[0, 0].set_title(\"Episode Rewards\")\n",
        "    axes[0, 0].set_xlabel(\"Episode\")\n",
        "    axes[0, 0].set_ylabel(\"Reward\")\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "    # Episode Steps\n",
        "    axes[0, 1].plot(steps, alpha=0.3, label=\"Raw\", color='green')\n",
        "    if len(steps) >= window:\n",
        "        smooth_steps = moving_average(steps, window)\n",
        "        axes[0, 1].plot(range(window-1, len(steps)), smooth_steps,\n",
        "                        label=f\"Moving Avg ({window})\", linewidth=2, color='orange')\n",
        "    axes[0, 1].set_title(\"Episode Length\")\n",
        "    axes[0, 1].set_xlabel(\"Episode\")\n",
        "    axes[0, 1].set_ylabel(\"Steps\")\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "    # Training Loss\n",
        "    if losses:\n",
        "        axes[1, 0].plot(losses, alpha=0.6, color='purple')\n",
        "        if len(losses) >= window:\n",
        "            smooth_loss = moving_average(losses, window)\n",
        "            axes[1, 0].plot(range(window-1, len(losses)), smooth_loss,\n",
        "                            label=f\"Moving Avg ({window})\", linewidth=2, color='red')\n",
        "            axes[1, 0].legend()\n",
        "    axes[1, 0].set_title(\"Training Loss\")\n",
        "    axes[1, 0].set_xlabel(\"Episode\")\n",
        "    axes[1, 0].set_ylabel(\"Loss\")\n",
        "    axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "    # Epsilon Decay (for DQN agents)\n",
        "    if epsilons:\n",
        "        axes[1, 1].plot(epsilons, color='brown', linewidth=2)\n",
        "        axes[1, 1].set_title(\"Exploration Rate (Epsilon)\")\n",
        "        axes[1, 1].set_xlabel(\"Episode\")\n",
        "        axes[1, 1].set_ylabel(\"Epsilon\")\n",
        "        axes[1, 1].grid(alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 1].axis(\"off\")\n",
        "        axes[1, 1].text(0.5, 0.5, \"No Epsilon Data\\n(Non-DQN Agent)\", \n",
        "                       ha='center', va='center', transform=axes[1, 1].transAxes,\n",
        "                       fontsize=12, style='italic')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_performance_summary(agent):\n",
        "    \"\"\"Print performance summary for a trained agent.\"\"\"\n",
        "    rewards = agent.episode_rewards\n",
        "    steps = agent.episode_lengths\n",
        "    losses = agent.losses\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PERFORMANCE SUMMARY - {agent.__class__.__name__.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    final_rewards = np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards)\n",
        "    best_rewards = max([np.mean(rewards[i:i+100]) for i in range(len(rewards)-99)]) if len(rewards) >= 100 else max(rewards)\n",
        "    final_steps = np.mean(steps[-100:]) if len(steps) >= 100 else np.mean(steps)\n",
        "    final_loss = np.mean(losses[-100:]) if len(losses) >= 100 else (np.mean(losses) if losses else 0)\n",
        "\n",
        "    print(f\"Final Performance (last 100 episodes):\")\n",
        "    print(f\"  Average Reward: {final_rewards:.3f}\")\n",
        "    print(f\"  Average Steps: {final_steps:.1f}\")\n",
        "    print(f\"  Training Loss: {final_loss:.4f}\")\n",
        "    print(f\"\\nBest Performance:\")\n",
        "    print(f\"  Best 100-episode Average Reward: {best_rewards:.3f}\")\n",
        "    print(f\"  Maximum Single Episode Reward: {max(rewards):.3f}\")\n",
        "    print(f\"  Total Episodes: {len(rewards)}\")\n",
        "    print(f\"  Total Training Steps: {sum(steps):,}\")\n",
        "        \n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "def run_agent(agent, save_model=True):\n",
        "    \"\"\"Train the agent, analyze results, and optionally save the model.\"\"\"\n",
        "    agent.train()\n",
        "    print_performance_summary(agent)\n",
        "    plot_training_results(agent)\n",
        "    \n",
        "    if save_model:\n",
        "        save_path = agent.save_model()\n",
        "        print(f\"\\nModel saved successfully!\")\n",
        "    \n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPupfarRiqrE",
      "metadata": {
        "id": "ZPupfarRiqrE"
      },
      "source": [
        "## DoubleDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "NfngpxZwjvYR",
      "metadata": {
        "id": "NfngpxZwjvYR"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_actions, feature_dim=128, input_size=84):\n",
        "        super().__init__()\n",
        "        self.encoder = MiniGridCNN(output_dim=feature_dim, input_size=input_size)\n",
        "        self.q_head = nn.Linear(feature_dim, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        return self.q_head(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "siPTq0Qc7x6E",
      "metadata": {
        "id": "siPTq0Qc7x6E"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.stack(states).to(device),\n",
        "            torch.stack(actions).to(device),\n",
        "            torch.stack(rewards).to(device),\n",
        "            torch.stack(next_states).to(device),\n",
        "            torch.stack(dones).to(device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "wvpLSRPg-uBz",
      "metadata": {
        "id": "wvpLSRPg-uBz"
      },
      "outputs": [],
      "source": [
        "class DoubleDQNAgent(BaseAgent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_name,\n",
        "        num_episodes=1000,\n",
        "        max_steps=500,\n",
        "        gamma=0.99,\n",
        "        lr=1e-3,\n",
        "        batch_size=256,\n",
        "        target_update_freq=100,\n",
        "        replay_capacity=10_000,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.01,\n",
        "        epsilon_decay=0.995,\n",
        "    ):\n",
        "        super().__init__(env_name, num_episodes, max_steps)\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.replay_buffer = ReplayBuffer(capacity=replay_capacity)\n",
        "\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilons = []\n",
        "\n",
        "        self.policy_net = QNetwork(self.num_actions, input_size=self.obs_shape[0]).to(device)\n",
        "        self.target_net = QNetwork(self.num_actions, input_size=self.obs_shape[0]).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
        "        self.global_step = 0\n",
        "\n",
        "    def _get_algorithm_name(self):\n",
        "        return \"DoubleDQN\"\n",
        "\n",
        "    def _save_model(self, save_dir):\n",
        "        torch.save(self.policy_net.state_dict(), os.path.join(save_dir, \"policy_net.pt\"))\n",
        "        torch.save(self.target_net.state_dict(), os.path.join(save_dir, \"target_net.pt\"))\n",
        "\n",
        "    def get_additional_logs(self):\n",
        "        return {\"epsilons\": self.epsilons}\n",
        "    \n",
        "    def _get_additional_logs_str(self):\n",
        "        \"\"\"Return additional logs as a formatted string for printing.\"\"\"\n",
        "        return f\" | Epsilon: {self.epsilon:.4f}\"\n",
        "    \n",
        "    def _select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return torch.tensor([random.randint(0, self.num_actions - 1)], device=device)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state.unsqueeze(0))\n",
        "            return q_values.argmax(dim=1)\n",
        "\n",
        "    def _train_episode(self, episode):\n",
        "        obs, _ = self.env.reset()\n",
        "        state = self._preprocess(obs).to(device)\n",
        "        total_reward, steps = 0, 0\n",
        "        episode_loss = 0.0\n",
        "        loss_count = 0\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            action = self._select_action(state)\n",
        "            next_obs, reward, terminated, truncated, _ = self.env.step(action.item())\n",
        "            next_state = self._preprocess(next_obs).to(device)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            self.replay_buffer.push((\n",
        "                state,\n",
        "                torch.tensor([action.item()], dtype=torch.long, device=device),\n",
        "                torch.tensor([reward], dtype=torch.float32, device=device),\n",
        "                next_state,\n",
        "                torch.tensor([done], dtype=torch.float32, device=device),\n",
        "            ))\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            self.global_step += 1\n",
        "\n",
        "            if len(self.replay_buffer) >= self.batch_size:\n",
        "                states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "                q_values = self.policy_net(states).gather(1, actions.view(-1, 1)).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)\n",
        "                    next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze()\n",
        "                    targets = rewards.squeeze() + self.gamma * next_q_values * (1 - dones.squeeze())\n",
        "\n",
        "                loss = F.mse_loss(q_values, targets)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                episode_loss += loss.item()\n",
        "                loss_count += 1\n",
        "\n",
        "            if self.global_step % self.target_update_freq == 0:\n",
        "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        self.epsilons.append(self.epsilon)\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "        avg_loss = episode_loss / loss_count if loss_count > 0 else 0.0\n",
        "                \n",
        "        return total_reward, steps, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "tw6MnzJ_AXuD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "tw6MnzJ_AXuD",
        "outputId": "0fcf0ed0-59db-4fe8-af8a-04810ea79276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING DOUBLEDQN ON MINIGRID-DYNAMIC-OBSTACLES-8X8-V0\n",
            "============================================================\n",
            "Episodes: 1000 | Max Steps: 500\n",
            "Observation Shape: (56, 56, 3) | Action Space: 3\n",
            "============================================================\n",
            "[DoubleDQN] Ep    0 | Reward:  -1.00 | Avg Reward:  -1.00 | Steps:  22 | Avg Steps:  22.0 | Loss: 0.0000 | Epsilon: 0.9950\n",
            "[DoubleDQN] Ep  100 | Reward:  -1.00 | Avg Reward:  -1.00 | Steps:   4 | Avg Steps:  13.1 | Loss: 0.0050 | Epsilon: 0.6027\n",
            "[DoubleDQN] Ep  200 | Reward:  -1.00 | Avg Reward:  -1.00 | Steps:  87 | Avg Steps:  26.0 | Loss: 0.0001 | Epsilon: 0.3651\n",
            "[DoubleDQN] Ep  300 | Reward:  -1.00 | Avg Reward:  -1.00 | Steps:  52 | Avg Steps:  42.1 | Loss: 0.0000 | Epsilon: 0.2212\n",
            "[DoubleDQN] Ep  400 | Reward:  -1.00 | Avg Reward:  -1.00 | Steps:  85 | Avg Steps:  41.5 | Loss: 0.0000 | Epsilon: 0.1340\n",
            "[DoubleDQN] Ep  500 | Reward:  -1.00 | Avg Reward:  -0.92 | Steps:  67 | Avg Steps: 104.9 | Loss: 0.0000 | Epsilon: 0.0812\n",
            "[DoubleDQN] Ep  600 | Reward:  -1.00 | Avg Reward:  -0.34 | Steps:  27 | Avg Steps:  78.4 | Loss: 0.0018 | Epsilon: 0.0492\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m DoubleDQNAgent(env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMiniGrid-Dynamic-Obstacles-8x8-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[29], line 103\u001b[0m, in \u001b[0;36mrun_agent\u001b[0;34m(agent, save_model)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_agent\u001b[39m(agent, save_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the agent, analyze results, and optionally save the model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     print_performance_summary(agent)\n\u001b[1;32m    105\u001b[0m     plot_training_results(agent)\n",
            "Cell \u001b[0;32mIn[28], line 73\u001b[0m, in \u001b[0;36mBaseAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_episodes):\n\u001b[0;32m---> 73\u001b[0m     reward, steps, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths\u001b[38;5;241m.\u001b[39mappend(steps)\n",
            "Cell \u001b[0;32mIn[37], line 94\u001b[0m, in \u001b[0;36mDoubleDQNAgent._train_episode\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(q_values, targets)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 94\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     97\u001b[0m episode_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "agent = DoubleDQNAgent(env_name=\"MiniGrid-Dynamic-Obstacles-8x8-v0\")\n",
        "run_agent(agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FTNeYSHt43HK",
      "metadata": {
        "id": "FTNeYSHt43HK"
      },
      "source": [
        "## REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3CzdKxPu5Lr7",
      "metadata": {
        "id": "3CzdKxPu5Lr7"
      },
      "outputs": [],
      "source": [
        "class ReinforcePolicy(nn.Module):\n",
        "    def __init__(self, num_actions, input_size=84, feature_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = MiniGridCNN(output_dim=feature_dim, input_size=input_size)\n",
        "        self.action_head = nn.Linear(feature_dim, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        logits = self.action_head(features)\n",
        "        return torch.distributions.Categorical(logits=logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "nxNUQyTV5O0_",
      "metadata": {
        "id": "nxNUQyTV5O0_"
      },
      "outputs": [],
      "source": [
        "class REINFORCEAgent(BaseAgent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_name,\n",
        "        num_episodes=1000,\n",
        "        max_steps=500,\n",
        "        gamma=0.99,\n",
        "        lr=1e-3,\n",
        "        entropy_coeff=0.01,\n",
        "    ):\n",
        "        super().__init__(env_name, num_episodes, max_steps)\n",
        "        self.gamma = gamma\n",
        "        self.entropy_coeff = entropy_coeff\n",
        "\n",
        "        self.policy = ReinforcePolicy(self.num_actions, input_size=self.obs_shape[0]).to(device)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    def _get_algorithm_name(self):\n",
        "        return \"REINFORCE\"\n",
        "\n",
        "    def _save_model(self, save_dir):\n",
        "        torch.save(self.policy.state_dict(), os.path.join(save_dir, \"policy.pt\"))\n",
        "\n",
        "    def _train_episode(self, episode):\n",
        "        obs, _ = self.env.reset()\n",
        "        state = self._preprocess(obs).to(device)\n",
        "\n",
        "        log_probs, rewards, entropies = [], [], []\n",
        "        total_reward, steps = 0, 0\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            dist = self.policy(state)\n",
        "            action = dist.sample()\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "            entropies.append(dist.entropy())\n",
        "\n",
        "            next_obs, reward, terminated, truncated, _ = self.env.step(action.item())\n",
        "            next_state = self._preprocess(next_obs).to(device)\n",
        "\n",
        "            rewards.append(reward)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # Compute returns (discounted sum of rewards)\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        if not returns:\n",
        "            return total_reward, steps, 0.0\n",
        "\n",
        "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        entropies = torch.stack(entropies)\n",
        "\n",
        "        loss = -(log_probs * returns).sum() - self.entropy_coeff * entropies.sum()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return total_reward, steps, loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f6c64072",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING REINFORCE ON MINIGRID-DYNAMIC-OBSTACLES-6X6-V0\n",
            "============================================================\n",
            "Episodes: 500 | Max Steps: 500\n",
            "Observation Shape: (56, 56, 3) | Action Space: 3\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (64x9 and 576x128)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test REINFORCE agent\u001b[39;00m\n\u001b[1;32m      2\u001b[0m reinforce_agent \u001b[38;5;241m=\u001b[39m REINFORCEAgent(env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMiniGrid-Dynamic-Obstacles-6x6-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinforce_agent\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[29], line 103\u001b[0m, in \u001b[0;36mrun_agent\u001b[0;34m(agent, save_model)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_agent\u001b[39m(agent, save_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the agent, analyze results, and optionally save the model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     print_performance_summary(agent)\n\u001b[1;32m    105\u001b[0m     plot_training_results(agent)\n",
            "Cell \u001b[0;32mIn[28], line 73\u001b[0m, in \u001b[0;36mBaseAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_episodes):\n\u001b[0;32m---> 73\u001b[0m     reward, steps, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths\u001b[38;5;241m.\u001b[39mappend(steps)\n",
            "Cell \u001b[0;32mIn[35], line 32\u001b[0m, in \u001b[0;36mREINFORCEAgent._train_episode\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     29\u001b[0m total_reward, steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps):\n\u001b[0;32m---> 32\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     34\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(dist\u001b[38;5;241m.\u001b[39mlog_prob(action))\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[34], line 8\u001b[0m, in \u001b[0;36mReinforcePolicy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_head(features)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[27], line 25\u001b[0m, in \u001b[0;36mMiniGridCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x9 and 576x128)"
          ]
        }
      ],
      "source": [
        "# Test REINFORCE agent\n",
        "reinforce_agent = REINFORCEAgent(env_name=\"MiniGrid-Dynamic-Obstacles-6x6-v0\", num_episodes=500)\n",
        "run_agent(reinforce_agent)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
