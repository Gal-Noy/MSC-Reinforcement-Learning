{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378b4272",
   "metadata": {
    "id": "378b4272"
   },
   "source": [
    "# Reinforcement Learning 2025 - Final Assignment\n",
    "\n",
    "**Authors:** Amit Ezer, Gal Yaacov Noy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e0ea80ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ea80ba",
    "outputId": "bf20ad66-328b-4ab9-91f9-78c6afee9e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minigrid in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: gymnasium in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from minigrid) (2.2.6)\n",
      "Requirement already satisfied: pygame>=2.4.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from minigrid) (2.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install minigrid gymnasium matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c3d95ce4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3d95ce4",
    "outputId": "cd7b4b47-207b-400f-dfbc-55cf0966bab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper, RGBImgPartialObsWrapper\n",
    "from collections import deque\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "pHPS2JIMNIhu",
   "metadata": {
    "id": "pHPS2JIMNIhu"
   },
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    \"\"\"\n",
    "    Convert RGB observation to tensor and normalize to [0,1] range.\n",
    "    Input: RGB image array (H, W, 3)\n",
    "    Output: tensor (C, H, W) normalized to [0,1]\n",
    "    \"\"\"\n",
    "    return torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "def create_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = RGBImgPartialObsWrapper(env)\n",
    "    env = ImgObsWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6YJeLgqj94t6",
   "metadata": {
    "id": "6YJeLgqj94t6"
   },
   "outputs": [],
   "source": [
    "class MiniGridCNN(nn.Module):\n",
    "    def __init__(self, output_dim=128, input_channels=3, input_size=84):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the output size after the conv layers by passing a dummy input.\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, input_size, input_size)\n",
    "            conv_out_size = self.conv(dummy_input).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZPupfarRiqrE",
   "metadata": {
    "id": "ZPupfarRiqrE"
   },
   "source": [
    "## DoubleDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "NfngpxZwjvYR",
   "metadata": {
    "id": "NfngpxZwjvYR"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_actions, feature_dim=128, input_size=84):\n",
    "        super().__init__()\n",
    "        self.encoder = MiniGridCNN(output_dim=feature_dim, input_size=input_size)\n",
    "        self.q_head = nn.Linear(feature_dim, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.q_head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "siPTq0Qc7x6E",
   "metadata": {
    "id": "siPTq0Qc7x6E"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.stack(states).to(device),\n",
    "            torch.stack(actions).to(device),\n",
    "            torch.stack(rewards).to(device),\n",
    "            torch.stack(next_states).to(device),\n",
    "            torch.stack(dones).to(device),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1qsTwvnO70OF",
   "metadata": {
    "id": "1qsTwvnO70OF"
   },
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy strategy:\n",
    "    - With probability `epsilon`, a random action is chosen (exploration).\n",
    "    - With probability `1 - epsilon`, the action with the highest Q-value is chosen (exploitation).\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return torch.tensor([random.randint(0, num_actions - 1)], device=device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state.unsqueeze(0))\n",
    "        return q_values.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "rUegirpw78me",
   "metadata": {
    "id": "rUegirpw78me"
   },
   "outputs": [],
   "source": [
    "def train_double_dqn(\n",
    "    env_name=\"MiniGrid-Dynamic-Obstacles-16x16-v0\",\n",
    "    num_episodes=5000,\n",
    "    max_steps=500,             \n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr=1e-4,\n",
    "    target_update_freq=500,\n",
    "    replay_capacity=50_000,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay=0.995\n",
    "):\n",
    "    env = create_env(env_name)\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "    obs, _ = env.reset()\n",
    "    input_size = obs.shape[0]\n",
    "\n",
    "    # Models\n",
    "    policy_net = QNetwork(num_actions, input_size=input_size).to(device)\n",
    "    target_net = QNetwork(num_actions, input_size=input_size).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=replay_capacity)\n",
    "\n",
    "    # Tracking\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    epsilons = []\n",
    "    global_step = 0\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING DOUBLE DQN ON {env_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Episodes: {num_episodes} | Batch Size: {batch_size} | Learning Rate: {lr}\")\n",
    "    print(f\"Gamma: {gamma} | Target Update: {target_update_freq} | Replay Capacity: {replay_capacity:,}\")\n",
    "    print(f\"Epsilon: {epsilon_start} -> {epsilon_end} (decay: {epsilon_decay})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess(obs).to(device)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action = select_action(policy_net, state, epsilon, num_actions)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            next_state = preprocess(next_obs).to(device)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Store transition\n",
    "            transition = (\n",
    "                state.cpu(),\n",
    "                torch.tensor([action.item()], dtype=torch.long),\n",
    "                torch.tensor([reward], dtype=torch.float32),\n",
    "                next_state.cpu(),\n",
    "                torch.tensor([done], dtype=torch.float32)\n",
    "            )\n",
    "            replay_buffer.push(transition)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Train step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Current Q-values for taken actions\n",
    "                q_values = policy_net(states).gather(1, actions.view(-1, 1)).squeeze()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Double DQN: use policy network to select actions, target network to evaluate\n",
    "                    next_actions = policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "                    next_q_values = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "                    targets = rewards.squeeze() + gamma * next_q_values * (1 - dones.squeeze())\n",
    "\n",
    "                loss = F.mse_loss(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Learn\n",
    "            if global_step % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilons.append(epsilon)\n",
    "\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            recent_rewards = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
    "            recent_steps = np.mean(episode_lengths[-50:]) if len(episode_lengths) >= 50 else np.mean(episode_lengths)\n",
    "            recent_loss = np.mean(losses[-100:]) if len(losses) >= 100 else (np.mean(losses) if losses else 0)\n",
    "            print(f\"[Double DQN] Ep {episode:4d} | Reward: {total_reward:6.2f} | Avg Reward: {recent_rewards:6.2f} | Steps: {steps:3d} | Avg Steps: {recent_steps:5.1f} | Epsilon: {epsilon:.3f} | Loss: {recent_loss:.4f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING COMPLETED!\")\n",
    "    print(f\"Final Average Reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.3f}\")\n",
    "    print(f\"Final Average Steps (last 100 episodes): {np.mean(episode_lengths[-100:]):.1f}\")\n",
    "    print(f\"Total Training Steps: {global_step:,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        \"name\": \"Double DQN\",\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"steps\": episode_lengths,\n",
    "        \"losses\": losses,\n",
    "        \"epsilons\": epsilons\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b3e35",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "433a3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_actions, input_size=84, feature_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = MiniGridCNN(output_dim=feature_dim, input_size=input_size)\n",
    "        self.actor = nn.Linear(feature_dim, num_actions)\n",
    "        self.critic = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.actor(features), self.critic(features)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits, _ = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), dist.entropy()\n",
    "\n",
    "    def evaluate(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_probs, values.squeeze(), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ab636175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    \"\"\"Compute Generalized Advantage Estimation (GAE) for advantage calculation.\"\"\"\n",
    "    advs = []\n",
    "    gae = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advs.insert(0, gae)\n",
    "    return torch.tensor(advs, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1acad3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(\n",
    "    env_name=\"MiniGrid-Dynamic-Obstacles-16x16-v0\",\n",
    "    num_episodes=2000,\n",
    "    max_steps=500,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    clip_eps=0.2,\n",
    "    lr=2.5e-4,\n",
    "    update_epochs=4,\n",
    "    minibatch_size=64,\n",
    "    entropy_coef=0.01,\n",
    "    vf_coef=0.5\n",
    "):\n",
    "    env = create_env(env_name)\n",
    "    num_actions = env.action_space.n\n",
    "    obs, _ = env.reset()\n",
    "    input_size = obs.shape[0]\n",
    "\n",
    "    model = ActorCritic(num_actions=num_actions, input_size=input_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    episode_rewards, episode_lengths, losses = [], [], []\n",
    "    global_step = 0\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING PPO ON {env_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess(obs).to(device)\n",
    "\n",
    "        # Collect trajectory\n",
    "        log_probs, actions, rewards, values, dones, states = [], [], [], [], [], []\n",
    "        for _ in range(max_steps):\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _ = model.get_action(state)\n",
    "                _, value = model(state)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = preprocess(next_obs).to(device)\n",
    "\n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(torch.tensor(action).to(device))\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward).to(device))\n",
    "            values.append(value.squeeze())\n",
    "            dones.append(torch.tensor(done).float().to(device))\n",
    "\n",
    "            state = next_state\n",
    "            global_step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Bootstrap final value\n",
    "        with torch.no_grad():\n",
    "            _, next_value = model(state)\n",
    "        values.append(next_value.squeeze())\n",
    "\n",
    "        # Compute advantages and returns\n",
    "        advantages = compute_gae(rewards, values, dones, gamma, lam)\n",
    "        returns = advantages + torch.stack(values[:-1])\n",
    "        dataset = list(zip(states, actions, log_probs, returns, advantages))\n",
    "\n",
    "        # PPO update step\n",
    "        for _ in range(update_epochs):\n",
    "            random.shuffle(dataset)\n",
    "            for i in range(0, len(dataset), minibatch_size):\n",
    "                batch = dataset[i:i + minibatch_size]\n",
    "                if len(batch) < minibatch_size:\n",
    "                    continue\n",
    "\n",
    "                b_states, b_actions, b_old_log_probs, b_returns, b_advs = zip(*batch)\n",
    "                b_states = torch.stack(b_states).to(device)\n",
    "                b_actions = torch.stack(b_actions).to(device)\n",
    "                b_old_log_probs = torch.stack(b_old_log_probs).to(device)\n",
    "                b_returns = torch.stack(b_returns).to(device)\n",
    "                b_advs = torch.stack(b_advs).to(device)\n",
    "\n",
    "                log_probs, values, entropy = model.evaluate(b_states, b_actions)\n",
    "                ratio = torch.exp(log_probs - b_old_log_probs)\n",
    "\n",
    "                surr1 = ratio * b_advs\n",
    "                surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * b_advs\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                value_loss = F.mse_loss(values, b_returns)\n",
    "                loss = policy_loss + vf_coef * value_loss - entropy_coef * entropy.mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        # Track metrics\n",
    "        total_reward = sum([r.item() for r in rewards])\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(len(rewards))\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            avg_len = np.mean(episode_lengths[-50:])\n",
    "            avg_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            print(f\"[PPO] Ep {episode:4d} | Reward: {total_reward:.2f} | Avg: {avg_reward:.2f} | Steps: {avg_len:.1f} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"\\n{'='*60}\\nTRAINING COMPLETED!\\n{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        \"name\": \"PPO\",\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"steps\": episode_lengths,\n",
    "        \"losses\": losses,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "F89JPoi9mJC2",
   "metadata": {
    "id": "F89JPoi9mJC2"
   },
   "outputs": [],
   "source": [
    "def plot_training_results(logs, window=50, title=\"Double DQN Training Results\"):\n",
    "    \"\"\"Plot training results with moving averages like in midterm.\"\"\"\n",
    "    def moving_average(data, window):\n",
    "        if len(data) < window:\n",
    "            return data\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Episode Rewards\n",
    "    axes[0, 0].plot(logs[\"rewards\"], alpha=0.3, label=\"Raw\", color='blue')\n",
    "    if len(logs[\"rewards\"]) >= window:\n",
    "        smooth_rewards = moving_average(logs[\"rewards\"], window)\n",
    "        axes[0, 0].plot(range(window-1, len(logs[\"rewards\"])), smooth_rewards,\n",
    "                       label=f\"Moving Avg ({window})\", linewidth=2, color='red')\n",
    "    axes[0, 0].set_title(\"Episode Rewards\")\n",
    "    axes[0, 0].set_xlabel(\"Episode\")\n",
    "    axes[0, 0].set_ylabel(\"Reward\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Episode Steps\n",
    "    axes[0, 1].plot(logs[\"steps\"], alpha=0.3, label=\"Raw\", color='green')\n",
    "    if len(logs[\"steps\"]) >= window:\n",
    "        smooth_steps = moving_average(logs[\"steps\"], window)\n",
    "        axes[0, 1].plot(range(window-1, len(logs[\"steps\"])), smooth_steps,\n",
    "                       label=f\"Moving Avg ({window})\", linewidth=2, color='orange')\n",
    "    axes[0, 1].set_title(\"Episode Length\")\n",
    "    axes[0, 1].set_xlabel(\"Episode\")\n",
    "    axes[0, 1].set_ylabel(\"Steps\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    # Training Loss\n",
    "    if logs[\"losses\"]:\n",
    "        axes[1, 0].plot(logs[\"losses\"], alpha=0.6, color='purple')\n",
    "        if len(logs[\"losses\"]) >= window:\n",
    "            smooth_loss = moving_average(logs[\"losses\"], window)\n",
    "            axes[1, 0].plot(range(window-1, len(logs[\"losses\"])), smooth_loss,\n",
    "                           label=f\"Moving Avg ({window})\", linewidth=2, color='red')\n",
    "            axes[1, 0].legend()\n",
    "    axes[1, 0].set_title(\"Training Loss\")\n",
    "    axes[1, 0].set_xlabel(\"Training Step\")\n",
    "    axes[1, 0].set_ylabel(\"TD Loss\")\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Epsilon Decay\n",
    "    if logs[\"epsilons\"]:\n",
    "        axes[1, 1].plot(logs[\"epsilons\"], color='brown', linewidth=2)\n",
    "        axes[1, 1].set_title(\"Exploration Rate (Epsilon)\")\n",
    "        axes[1, 1].set_xlabel(\"Episode\")\n",
    "        axes[1, 1].set_ylabel(\"Epsilon\")\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_performance_summary(logs):\n",
    "    \"\"\"Print performance summary like in midterm.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PERFORMANCE SUMMARY - {logs['name'].upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    final_rewards = np.mean(logs[\"rewards\"][-100:]) if len(logs[\"rewards\"]) >= 100 else np.mean(logs[\"rewards\"])\n",
    "    best_rewards = max([np.mean(logs[\"rewards\"][i:i+100]) for i in range(len(logs[\"rewards\"])-99)]) if len(logs[\"rewards\"]) >= 100 else max(logs[\"rewards\"])\n",
    "    final_steps = np.mean(logs[\"steps\"][-100:]) if len(logs[\"steps\"]) >= 100 else np.mean(logs[\"steps\"])\n",
    "    final_loss = np.mean(logs[\"losses\"][-1000:]) if len(logs[\"losses\"]) >= 1000 else (np.mean(logs[\"losses\"]) if logs[\"losses\"] else 0)\n",
    "\n",
    "    print(f\"Final Performance (last 100 episodes):\")\n",
    "    print(f\"  Average Reward: {final_rewards:.3f}\")\n",
    "    print(f\"  Average Steps: {final_steps:.1f}\")\n",
    "    print(f\"  Training Loss: {final_loss:.4f}\")\n",
    "    print(f\"\\nBest Performance:\")\n",
    "    print(f\"  Best 100-episode Average Reward: {best_rewards:.3f}\")\n",
    "    print(f\"  Total Episodes: {len(logs['rewards'])}\")\n",
    "    print(f\"  Total Training Steps: {len(logs['losses']):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'Double DQN': train_double_dqn,\n",
    "    'PPO': train_ppo\n",
    "}\n",
    "\n",
    "def run_experiment(name, **kwargs):\n",
    "    \"\"\"Run complete experiment: train agent, analyze, and plot.\"\"\"\n",
    "    # Train agent\n",
    "    logs = EXPERIMENTS[name](**kwargs)\n",
    "\n",
    "    # Analyze and visualize\n",
    "    print_performance_summary(logs)\n",
    "    plot_training_results(logs)\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lN3dHUCIqiB2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "lN3dHUCIqiB2",
    "outputId": "bb7f50ac-1182-4b41-fc75-86e06f071eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING DOUBLE DQN ON MINIGRID-DYNAMIC-OBSTACLES-5X5-V0\n",
      "============================================================\n",
      "Episodes: 1000 | Batch Size: 64 | Learning Rate: 0.0001\n",
      "Gamma: 0.99 | Target Update: 500 | Replay Capacity: 50,000\n",
      "Epsilon: 1.0 -> 0.05 (decay: 0.995)\n",
      "============================================================\n",
      "[Double DQN] Ep    0 | Reward:  -1.00 | Avg Reward:  -1.00 | Steps:   3 | Avg Steps:   3.0 | Epsilon: 0.995 | Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "dqn_logs = run_experiment('Double DQN', env_name=\"MiniGrid-Dynamic-Obstacles-5x5-v0\", num_episodes=1000, max_steps=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2becb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING PPO ON MINIGRID-DYNAMIC-OBSTACLES-16X16-V0\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING PPO ON MINIGRID-DYNAMIC-OBSTACLES-16X16-V0\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x49 and 3136x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppo_logs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[107], line 88\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run complete experiment: train agent, analyze, and plot.\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mEXPERIMENTS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Analyze and visualize\u001b[39;00m\n\u001b[1;32m     91\u001b[0m print_performance_summary(logs)\n",
      "Cell \u001b[0;32mIn[106], line 37\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(env_name, num_episodes, max_steps, gamma, lam, clip_eps, lr, update_epochs, minibatch_size, entropy_coef, vf_coef)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 37\u001b[0m         action, log_prob, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m         _, value \u001b[38;5;241m=\u001b[39m model(state)\n\u001b[1;32m     40\u001b[0m     next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[104], line 13\u001b[0m, in \u001b[0;36mActorCritic.get_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m---> 13\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[1;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[0;32mIn[104], line 9\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(features), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(features)\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[99], line 27\u001b[0m, in \u001b[0;36mMiniGridCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x49 and 3136x128)"
     ]
    }
   ],
   "source": [
    "# ppo_logs = run_experiment(\n",
    "#     'PPO'\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
