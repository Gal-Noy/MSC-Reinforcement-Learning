{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378b4272",
   "metadata": {
    "id": "378b4272"
   },
   "source": [
    "# Reinforcement Learning 2025 - Final Assignment\n",
    "\n",
    "**Authors:** Amit Ezer, Gal Yaacov Noy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "e0ea80ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ea80ba",
    "outputId": "bf20ad66-328b-4ab9-91f9-78c6afee9e95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8045.30s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minigrid in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: gymnasium in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from minigrid) (2.2.6)\n",
      "Requirement already satisfied: pygame>=2.4.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from minigrid) (2.6.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/galnoy/git-projects/MSC-Reinforcement-Learning/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install minigrid gymnasium matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "c3d95ce4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3d95ce4",
    "outputId": "cd7b4b47-207b-400f-dfbc-55cf0966bab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper, RGBImgPartialObsWrapper\n",
    "from collections import deque\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "pHPS2JIMNIhu",
   "metadata": {
    "id": "pHPS2JIMNIhu"
   },
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    \"\"\"\n",
    "    Convert RGB observation to tensor and normalize to [0,1] range.\n",
    "    Input: RGB image array (H, W, 3)\n",
    "    Output: tensor (C, H, W) normalized to [0,1]\n",
    "    \"\"\"\n",
    "    return torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "def create_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = RGBImgPartialObsWrapper(env)\n",
    "    env = ImgObsWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6YJeLgqj94t6",
   "metadata": {
    "id": "6YJeLgqj94t6"
   },
   "outputs": [],
   "source": [
    "class MiniGridCNN(nn.Module):\n",
    "    def __init__(self, output_dim=128, input_channels=3, input_size=56):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the output size after the conv layers by passing a dummy input.\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, input_size, input_size)\n",
    "            conv_out_size = self.conv(dummy_input).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZPupfarRiqrE",
   "metadata": {
    "id": "ZPupfarRiqrE"
   },
   "source": [
    "## DoubleDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "NfngpxZwjvYR",
   "metadata": {
    "id": "NfngpxZwjvYR"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_actions, feature_dim=128, input_size=56):\n",
    "        super().__init__()\n",
    "        self.encoder = MiniGridCNN(output_dim=feature_dim, input_size=input_size)\n",
    "        self.q_head = nn.Linear(feature_dim, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.q_head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "siPTq0Qc7x6E",
   "metadata": {
    "id": "siPTq0Qc7x6E"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.stack(states).to(device),\n",
    "            torch.stack(actions).to(device),\n",
    "            torch.stack(rewards).to(device),\n",
    "            torch.stack(next_states).to(device),\n",
    "            torch.stack(dones).to(device),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "1qsTwvnO70OF",
   "metadata": {
    "id": "1qsTwvnO70OF"
   },
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy strategy:\n",
    "    - With probability `epsilon`, a random action is chosen (exploration).\n",
    "    - With probability `1 - epsilon`, the action with the highest Q-value is chosen (exploitation).\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return torch.tensor([random.randint(0, num_actions - 1)], device=device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state.unsqueeze(0))\n",
    "        return q_values.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "rUegirpw78me",
   "metadata": {
    "id": "rUegirpw78me"
   },
   "outputs": [],
   "source": [
    "def train_double_dqn(\n",
    "    env_name=\"MiniGrid-Dynamic-Obstacles-16x16-v0\",\n",
    "    num_episodes=5000,\n",
    "    max_steps=500,             \n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr=1e-4,\n",
    "    target_update_freq=500,\n",
    "    replay_capacity=50_000,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay=0.995\n",
    "):\n",
    "    env = create_env(env_name)\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "    obs, _ = env.reset()\n",
    "    input_size = obs.shape[0]\n",
    "\n",
    "    # Models\n",
    "    policy_net = QNetwork(num_actions, input_size=input_size).to(device)\n",
    "    target_net = QNetwork(num_actions, input_size=input_size).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=replay_capacity)\n",
    "\n",
    "    # Tracking\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    epsilons = []\n",
    "    global_step = 0\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING DOUBLE DQN ON {env_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Episodes: {num_episodes} | Batch Size: {batch_size} | Learning Rate: {lr}\")\n",
    "    print(f\"Gamma: {gamma} | Target Update: {target_update_freq} | Replay Capacity: {replay_capacity:,}\")\n",
    "    print(f\"Epsilon: {epsilon_start} -> {epsilon_end} (decay: {epsilon_decay})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess(obs).to(device)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action = select_action(policy_net, state, epsilon, num_actions)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            next_state = preprocess(next_obs).to(device)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Store transition\n",
    "            transition = (\n",
    "                state.cpu(),\n",
    "                torch.tensor([action.item()], dtype=torch.long),\n",
    "                torch.tensor([reward], dtype=torch.float32),\n",
    "                next_state.cpu(),\n",
    "                torch.tensor([done], dtype=torch.float32)\n",
    "            )\n",
    "            replay_buffer.push(transition)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Train step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Current Q-values for taken actions\n",
    "                q_values = policy_net(states).gather(1, actions.view(-1, 1)).squeeze()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Double DQN: use policy network to select actions, target network to evaluate\n",
    "                    next_actions = policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "                    next_q_values = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "                    targets = rewards.squeeze() + gamma * next_q_values * (1 - dones.squeeze())\n",
    "\n",
    "                loss = F.mse_loss(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Learn\n",
    "            if global_step % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilons.append(epsilon)\n",
    "\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            recent_rewards = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
    "            recent_steps = np.mean(episode_lengths[-50:]) if len(episode_lengths) >= 50 else np.mean(episode_lengths)\n",
    "            recent_loss = np.mean(losses[-100:]) if len(losses) >= 100 else (np.mean(losses) if losses else 0)\n",
    "            print(f\"[Double DQN] Ep {episode:4d} | Reward: {total_reward:6.2f} | Avg Reward: {recent_rewards:6.2f} | Steps: {steps:3d} | Avg Steps: {recent_steps:5.1f} | Epsilon: {epsilon:.3f} | Loss: {recent_loss:.4f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING COMPLETED!\")\n",
    "    print(f\"Final Average Reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.3f}\")\n",
    "    print(f\"Final Average Steps (last 100 episodes): {np.mean(episode_lengths[-100:]):.1f}\")\n",
    "    print(f\"Total Training Steps: {global_step:,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        \"name\": \"Double DQN\",\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"steps\": episode_lengths,\n",
    "        \"losses\": losses,\n",
    "        \"epsilons\": epsilons\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b3e35",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "433a3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_actions, input_size=84, feature_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = MiniGridCNN(output_dim=feature_dim, input_size=input_size)\n",
    "        self.actor = nn.Linear(feature_dim, num_actions)\n",
    "        self.critic = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.actor(features), self.critic(features)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits, _ = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), dist.entropy()\n",
    "\n",
    "    def evaluate(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return log_probs, values.squeeze(), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "ab636175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    \"\"\"Compute Generalized Advantage Estimation (GAE) for advantage calculation.\"\"\"\n",
    "    advs = []\n",
    "    gae = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advs.insert(0, gae)\n",
    "    return torch.tensor(advs, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "1acad3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(\n",
    "    env_name=\"MiniGrid-Dynamic-Obstacles-16x16-v0\",\n",
    "    num_episodes=2000,\n",
    "    max_steps=500,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    clip_eps=0.2,\n",
    "    lr=2.5e-4,\n",
    "    update_epochs=4,\n",
    "    minibatch_size=64,\n",
    "    entropy_coef=0.01,\n",
    "    vf_coef=0.5\n",
    "):\n",
    "    env = create_env(env_name)\n",
    "    num_actions = env.action_space.n\n",
    "    obs, _ = env.reset()\n",
    "    input_size = obs.shape[0]\n",
    "\n",
    "    model = ActorCritic(num_actions=num_actions, input_size=input_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    episode_rewards, episode_lengths, losses = [], [], []\n",
    "    global_step = 0\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING PPO ON {env_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess(obs).to(device)\n",
    "\n",
    "        # Collect trajectory\n",
    "        log_probs, actions, rewards, values, dones, states = [], [], [], [], [], []\n",
    "        for _ in range(max_steps):\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _ = model.get_action(state.unsqueeze(0))\n",
    "                _, value = model(state.unsqueeze(0))\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = preprocess(next_obs).to(device)\n",
    "\n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(torch.tensor(action).to(device))\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward).to(device))\n",
    "            values.append(value.squeeze())\n",
    "            dones.append(torch.tensor(done).float().to(device))\n",
    "\n",
    "            state = next_state\n",
    "            global_step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Bootstrap final value\n",
    "        with torch.no_grad():\n",
    "            _, next_value = model(state.unsqueeze(0))\n",
    "        values.append(next_value.squeeze())\n",
    "\n",
    "        # Compute advantages and returns\n",
    "        advantages = compute_gae(rewards, values, dones, gamma, lam)\n",
    "        returns = advantages + torch.stack(values[:-1])\n",
    "        dataset = list(zip(states, actions, log_probs, returns, advantages))\n",
    "\n",
    "        # PPO update step\n",
    "        for _ in range(update_epochs):\n",
    "            random.shuffle(dataset)\n",
    "            for i in range(0, len(dataset), minibatch_size):\n",
    "                batch = dataset[i:i + minibatch_size]\n",
    "                if len(batch) < minibatch_size:\n",
    "                    continue\n",
    "\n",
    "                b_states, b_actions, b_old_log_probs, b_returns, b_advs = zip(*batch)\n",
    "                b_states = torch.stack(b_states).to(device)\n",
    "                b_actions = torch.stack(b_actions).to(device)\n",
    "                b_old_log_probs = torch.stack(b_old_log_probs).to(device)\n",
    "                b_returns = torch.stack(b_returns).to(device)\n",
    "                b_advs = torch.stack(b_advs).to(device)\n",
    "\n",
    "                log_probs, values, entropy = model.evaluate(b_states, b_actions)\n",
    "                ratio = torch.exp(log_probs - b_old_log_probs)\n",
    "\n",
    "                surr1 = ratio * b_advs\n",
    "                surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * b_advs\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                value_loss = F.mse_loss(values, b_returns)\n",
    "                loss = policy_loss + vf_coef * value_loss - entropy_coef * entropy.mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        # Track metrics\n",
    "        total_reward = sum([r.item() for r in rewards])\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(len(rewards))\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            avg_len = np.mean(episode_lengths[-50:])\n",
    "            avg_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            print(f\"[PPO] Ep {episode:4d} | Reward: {total_reward:.2f} | Avg: {avg_reward:.2f} | Steps: {avg_len:.1f} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"\\n{'='*60}\\nTRAINING COMPLETED!\\n{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        \"name\": \"PPO\",\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"steps\": episode_lengths,\n",
    "        \"losses\": losses,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "F89JPoi9mJC2",
   "metadata": {
    "id": "F89JPoi9mJC2"
   },
   "outputs": [],
   "source": [
    "def plot_training_results(logs, window=50, title=\"Double DQN Training Results\"):\n",
    "    \"\"\"Plot training results with moving averages like in midterm.\"\"\"\n",
    "    def moving_average(data, window):\n",
    "        if len(data) < window:\n",
    "            return data\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Episode Rewards\n",
    "    axes[0, 0].plot(logs[\"rewards\"], alpha=0.3, label=\"Raw\", color='blue')\n",
    "    if len(logs[\"rewards\"]) >= window:\n",
    "        smooth_rewards = moving_average(logs[\"rewards\"], window)\n",
    "        axes[0, 0].plot(range(window-1, len(logs[\"rewards\"])), smooth_rewards,\n",
    "                       label=f\"Moving Avg ({window})\", linewidth=2, color='red')\n",
    "    axes[0, 0].set_title(\"Episode Rewards\")\n",
    "    axes[0, 0].set_xlabel(\"Episode\")\n",
    "    axes[0, 0].set_ylabel(\"Reward\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Episode Steps\n",
    "    axes[0, 1].plot(logs[\"steps\"], alpha=0.3, label=\"Raw\", color='green')\n",
    "    if len(logs[\"steps\"]) >= window:\n",
    "        smooth_steps = moving_average(logs[\"steps\"], window)\n",
    "        axes[0, 1].plot(range(window-1, len(logs[\"steps\"])), smooth_steps,\n",
    "                       label=f\"Moving Avg ({window})\", linewidth=2, color='orange')\n",
    "    axes[0, 1].set_title(\"Episode Length\")\n",
    "    axes[0, 1].set_xlabel(\"Episode\")\n",
    "    axes[0, 1].set_ylabel(\"Steps\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    # Training Loss\n",
    "    if logs[\"losses\"]:\n",
    "        axes[1, 0].plot(logs[\"losses\"], alpha=0.6, color='purple')\n",
    "        if len(logs[\"losses\"]) >= window:\n",
    "            smooth_loss = moving_average(logs[\"losses\"], window)\n",
    "            axes[1, 0].plot(range(window-1, len(logs[\"losses\"])), smooth_loss,\n",
    "                           label=f\"Moving Avg ({window})\", linewidth=2, color='red')\n",
    "            axes[1, 0].legend()\n",
    "    axes[1, 0].set_title(\"Training Loss\")\n",
    "    axes[1, 0].set_xlabel(\"Training Step\")\n",
    "    axes[1, 0].set_ylabel(\"TD Loss\")\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Epsilon Decay\n",
    "    if logs[\"epsilons\"]:\n",
    "        axes[1, 1].plot(logs[\"epsilons\"], color='brown', linewidth=2)\n",
    "        axes[1, 1].set_title(\"Exploration Rate (Epsilon)\")\n",
    "        axes[1, 1].set_xlabel(\"Episode\")\n",
    "        axes[1, 1].set_ylabel(\"Epsilon\")\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_performance_summary(logs):\n",
    "    \"\"\"Print performance summary like in midterm.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PERFORMANCE SUMMARY - {logs['name'].upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    final_rewards = np.mean(logs[\"rewards\"][-100:]) if len(logs[\"rewards\"]) >= 100 else np.mean(logs[\"rewards\"])\n",
    "    best_rewards = max([np.mean(logs[\"rewards\"][i:i+100]) for i in range(len(logs[\"rewards\"])-99)]) if len(logs[\"rewards\"]) >= 100 else max(logs[\"rewards\"])\n",
    "    final_steps = np.mean(logs[\"steps\"][-100:]) if len(logs[\"steps\"]) >= 100 else np.mean(logs[\"steps\"])\n",
    "    final_loss = np.mean(logs[\"losses\"][-1000:]) if len(logs[\"losses\"]) >= 1000 else (np.mean(logs[\"losses\"]) if logs[\"losses\"] else 0)\n",
    "\n",
    "    print(f\"Final Performance (last 100 episodes):\")\n",
    "    print(f\"  Average Reward: {final_rewards:.3f}\")\n",
    "    print(f\"  Average Steps: {final_steps:.1f}\")\n",
    "    print(f\"  Training Loss: {final_loss:.4f}\")\n",
    "    print(f\"\\nBest Performance:\")\n",
    "    print(f\"  Best 100-episode Average Reward: {best_rewards:.3f}\")\n",
    "    print(f\"  Total Episodes: {len(logs['rewards'])}\")\n",
    "    print(f\"  Total Training Steps: {len(logs['losses']):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'Double DQN': train_double_dqn,\n",
    "    'PPO': train_ppo\n",
    "}\n",
    "\n",
    "def run_experiment(name, **kwargs):\n",
    "    \"\"\"Run complete experiment: train agent, analyze, and plot.\"\"\"\n",
    "    # Train agent\n",
    "    logs = EXPERIMENTS[name](**kwargs)\n",
    "\n",
    "    # Analyze and visualize\n",
    "    print_performance_summary(logs)\n",
    "    plot_training_results(logs)\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "lN3dHUCIqiB2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "lN3dHUCIqiB2",
    "outputId": "bb7f50ac-1182-4b41-fc75-86e06f071eae"
   },
   "outputs": [],
   "source": [
    "# dqn_logs = run_experiment('Double DQN', env_name=\"MiniGrid-Dynamic-Obstacles-5x5-v0\", num_episodes=1000, max_steps=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2becb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING PPO ON MINIGRID-DYNAMIC-OBSTACLES-6X6-V0\n",
      "============================================================\n",
      "[PPO] Ep    0 | Reward: -1.00 | Avg: -1.00 | Steps: 8.0 | Loss: 0.0000\n",
      "[PPO] Ep  100 | Reward: -1.00 | Avg: -1.00 | Steps: 5.1 | Loss: 0.0000\n",
      "[PPO] Ep  200 | Reward: -1.00 | Avg: -1.00 | Steps: 5.2 | Loss: 0.0000\n",
      "[PPO] Ep  300 | Reward: -1.00 | Avg: -1.00 | Steps: 6.7 | Loss: 0.0000\n",
      "[PPO] Ep  400 | Reward: -1.00 | Avg: -0.96 | Steps: 6.5 | Loss: 0.0000\n",
      "[PPO] Ep  500 | Reward: -1.00 | Avg: -1.00 | Steps: 5.7 | Loss: 0.0000\n",
      "[PPO] Ep  600 | Reward: -1.00 | Avg: -1.00 | Steps: 5.6 | Loss: 0.0000\n",
      "[PPO] Ep  700 | Reward: -1.00 | Avg: -1.00 | Steps: 6.1 | Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "ppo_logs = run_experiment('PPO',\n",
    "                          env_name=\"MiniGrid-Dynamic-Obstacles-6x6-v0\",\n",
    "    num_episodes=3000,\n",
    "    max_steps=2048, \n",
    "    gamma=0.99,\n",
    "    lam=0.95, \n",
    "    clip_eps=0.2,\n",
    "    lr=3e-4, \n",
    "    update_epochs=10,\n",
    "    minibatch_size=64,\n",
    "    entropy_coef=0.01,\n",
    "    vf_coef=0.5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
